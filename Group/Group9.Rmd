---
title: "Group 9: Bootstrapping Regression"
subtitle: '*Analysis of Drinking Alcohol Habits Effect on Health condition in a National Sample of US Adults*'
author: "Daxuan Deng, Ming-Chen Lu, Ningyuan Wang"
date: "December 5, 2019"
output: 
  html_document:
  toc: true
  number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(root.dir = "C:\\Users\\Daxuan Deng\\Documents") 
                      #results = 'asis', message = FALSE, 
                      #fig.height = 4)
# libraries
library(data.table)
library(dplyr)
library(knitr)
library(tidyverse)
library(kableExtra)
library(Hmisc) # for read xpt datasets 
library(ggplot2)
```

## Introduction
Drinking alcohol is a common behavior among adults. People always think drinking alcohol is good for mood but bad for health. However, whether healthy people like drinking alcohol as well? Our group was curious about what factors affect the drinking habits for US healthy adults. 

The analysis aims to evalute the effect of some factors, including health condition, sex, age, education level and poverty income ratio (PIR), on the frequency of drinking alcohol in the past year among a nationally representative sample of US adults by using bootstrapping regression. Our analysis was conducted with R, Python and Matlab. 

## Data

### Data Source 
The analysis sample consisits of all adults in the National Health and Nutrition Examination Survey (NHANES) from 2005 - 2006. The National Health and Nutrition Examination Survey (NHANES) is a program of studies designed to assess the health and nutritional status of adults and children in the United States. The sample for the survey is selected to represent the U.S. population of all ages. 

For the purpose of the study, we limited the data to the samples age $\geqslant$ 21 years with self-reported good health.

### Analysis Variables

In this analysis, ALQ120Q (freuquency of drinking alcohol in the past 12 months) from *[Alcohol Use](https://wwwn.cdc.gov/Nchs/Nhanes/2005-2006/ALQ_D.htm)* was selected as our response variable, and the predictors were HSD010 (general health condition) from *[Current Health Status](https://wwwn.cdc.gov/Nchs/Nhanes/2005-2006/HSQ_D.htm#HSQ480)*; RIDAGEYR (age), RIAGENDR (gender), DMDEDUC2 (education), INDFMPIR (poverty income ratio) from *[Demographics Data](https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Demographics&CycleBeginYear=2005)*. The variable descriptions as follow.

```{r, echo= FALSE}
v = c("ALQ120Q", "HSD010", "RIDAGEYR", "RIAGENDR", "DMDEDUC2", "INDFMPIR")
d = c("How often drink alcohol over past 12 months",
      "General health condition", 
      "Age at Screening Adjudicated - Recode", 
      "Gender of the sample person: 1-Male, 2-Female",
      "Person Highest Education Level",
      "Family Poverty Income Ratio")
data.table::data.table(Variable = v, Description  = d) %>%
  knitr::kable(align = "l") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), position = "left")
```


The primary outcome was understanding how the frequency of drinking alchol relates to the health conditions. In addition, we also explained the how the demographical factors such as age, gender, education and PIR relate to drinking habits.

## Method

Empirical bootstrap is a resampling method for assessing uncertainty of estimator. It is typically used when the exact or asymptotic analytic solutions are unavailable or unsatisfactory. 

Generally, we draw samples with replacement from data set, and calculate the statistic we are interested in. Then, we repeat it $B$ times to generate the empirical distribution of estimator. With this distribution we could derive the variance of estimator.

For example, suppose the regression model is:
$$y \sim X\beta$$
where $y\in R^n$ is the response, $X$ is the $n \times (p+1)$ design matrix, and $\beta \in R^{p+1}$ is the coefficient. In normal setting, we assume that error are Gaussian. Under this assumption, we calculate sample variance of coefficient estimator $Var(\hat{\beta})$ using residual sum of squares (RSS):
$$\widehat{Var} (\hat{\beta}) = {\hat{\sigma}}^2(X'X)^{-1}$$
$${\hat{\sigma}}^2 = \frac{RSS}{n-p-1}$$
However, this could lead a bad result if data is skewed, because the outliers will force the ${\hat{\sigma}}$ become much bigger. To address this problem, we may use the residual bootstrap.

After running regression, we obtain coefficient estimate $\hat{\beta}$ and residuals:
$$r = y  - \hat{y} = y - X\hat{\beta}$$
which is also a vector in $R^n$. Then we sample $r_i^*$ from $\{r_1, r_2,...r_n\}$ with replacement for $i \in \{1,2,...,n\}$, and define
$$r^* = (r_1^*, r_2^*,...,r_n^*)'$$
$$y^* = X\hat{\beta} + r^*$$
In other word, we fix design matrix $X$, but generate a new response vector $y^*$ using the fitted value and the 'error' from resampling the residuals. Now we fit a new model:
$$y^* \sim X\beta$$
and we get a new coefficient estimate $\beta^*$. Repeating this process for $B$ times, we get $B$ bootstrap samples of $\hat{\beta}$:
$$\beta^{*(1)},\beta^{*(1)},...\beta^{*(B)}$$
Finally, we could calculate the sample variance of $\hat{\beta}$:
$$\bar{\beta}^* = \frac{1}{B}\sum_{i=1}^B \beta^{*(i)}$$
$$\widehat{Var}_B(\hat{\beta}) = \frac{1}{B}\sum_{i=1}^B (\beta^{*(i)}-\bar{\beta}^*)^2$$
Basically, that's the core analysis method we use in this project. To do parallel computation, we use different softwares: MATLAB 2018b, Python 3.6.9 and R version 3.6.1 to calculate respectively. Besides, in all the analysis, a two-tailed $P$ value less than 0.05 $(P < 0.05)$ was considered statistically significant.

## Core Analysis {.tabset .tabset_fade}

### R

```{r, eval=FALSE}
#library(reticulate)
#use_virtualenv(virtualenv = "r-reticulate")
```

#### a. Data Loading and Cleaning
Our data was obtained from NHANES in .XPT file extention.  We used R package "Hmisc" and compiled data to regular dataframes.  Then, we merged three datasets into one based on sequence id and reduced the variables as well as limited samples to participants over 20 years in healthy condition. In addition, missing values were dropped in this part and continuous variables were standardized for the porpose of regression analysis. Finally, there were 2791 samples in our dataset. 

```{r, message = FALSE, warning = FALSE, echo= FALSE}
# read in the datasets and select the variables 
alcohol = sasxport.get("https://wwwn.cdc.gov/Nchs/Nhanes/2005-2006/ALQ_D.XPT") %>% 
  select(seqn, alq120q) %>%
  filter(alq120q < 366) %>%
  drop_na()
health = sasxport.get("https://wwwn.cdc.gov/Nchs/Nhanes/2005-2006/HSQ_D.XPT") %>%
  select(seqn, hsd010) %>%
  filter(hsd010 <=3 ) %>%
  drop_na() # limit to the healthy samples
demography = sasxport.get("https://wwwn.cdc.gov/Nchs/Nhanes/2005-2006/DEMO_D.XPT") %>%
  filter(ridageyr >=21,dmdeduc2 <= 5 ) %>%
  transmute(seqn, age = ridageyr, gender = riagendr, education = dmdeduc2, pir = indfmpir ) %>%
  drop_na() # limit to people > 20 years
```

```{r}
# join above datasets
df = left_join(alcohol, health, by = "seqn") %>% 
  left_join(., demography, by = "seqn") %>%
  drop_na() %>%
  rename(drinks = alq120q, health = hsd010) # 2791*7
# convert variables to factors and rescale continous variables
df = df %>% 
  mutate_at(vars(seqn, health, gender, education), as.factor) %>%
  transmute(health, gender, education, drinks_std = scale(drinks),
         age_std = scale(age),
         pir_std = scale(pir)) 
summary(df)
```


#### b. OLS Model and Diagnosis

First, we fitted an OLS regression model with drinks as response and health condition, age, gender, education and PIR as predictors. The reference level of the model was the health condition good (the other two levels of health were "very good" and "excellent"). A summary of the model fit shows as follow. 
```{r}
# fit the model
lm1 = lm(drinks_std ~ relevel(health, ref = 3)  + age_std + gender + 
           education + pir_std, data = df) 
summary(lm1) 
```
However, with the model diagnostics, we noticed that the residuals failed the OLS assumption with equal variance residuals. 
```{r}
# model diagnostics 
res = lm1$residuals 
fit = lm1$fitted
par(mfrow=c(2,2))
plot(fit, res, xlab = "fitted", ylab = "Residuals") # unequal varlaince in residuals
qqnorm(res, ylab = "Residuals"); qqline(res) # non-normal residual
```


#### c. Bootstrapping Regression
Since the data failed the model assumption and possible soutions such as lognormal or Box Cox transformation of the response wre not appropriate. We consider used bootstrapping regression as an alternative. The theory of bootstrapping regression residuals were claimed in the methods section.

We sampled residuals 1000 times and then create a faked vector called "boot_y" as new response variable. and then refitted the model with fixed predictors as above and got distributions of beta estimates.  With new beta estimate and corresponding standard error, we did t test for each predictor and computed p values for terms in the model. Unfortunately, we did not see obvious imporvement based on p values to justify statistical significance between response and predicotrs variables. However, we concluded that we had robust standard error and relative p-values. 

```{r}
set.seed(506)
# construct bootstrap samples : res and new response y* 
n = nrow(df) # sample size
B = 1e3  # number of bootstrap samples 
boot_samples = sample(res, n * B, replace = TRUE)
dim(boot_samples) = c(n, B) # each column is a dataset
boot_y = matrix(fit, n, ncol(boot_samples)) + boot_samples # bootstrap y as faked response
# refit the model with bootstrap samples on y* and get the estimate of coefficents
lm_coef = function(y) lm(y ~ relevel(df$health, ref = 3)  + df$age_std + df$gender + df$education + df$pir_std)$coef
boot_lms = apply(boot_y, 2, lm_coef)
# apply(boot_lms, 1, hist) # check the distributions of beta hat
# hist(boot_lms[2,], main = "point estimate of health in excellent",
#      xlab = expression(hat(b)["excellent health"]))
# hist(boot_lms[3,], main = "point estimate of health in very good",
#      xlab = expression(hat(b)["very good health"]))
# compute standard error for each term 
se = sqrt(rowSums((boot_lms - rowMeans(boot_lms))^2) / B)
t =  lm1$coefficients / se
p = pt(abs(t), df = 1, lower.tail = F)
tb = cbind(lm1$coef, se, t, p) 
rownames(tb) =  c("Intercept", "Health: excellent", "Health: very good", "Age", "Gender", "Education: 2", "Education: 3", "Education: 4", "Education: 5", "PIR")
tb %>% 
  knitr::kable(align = "c", col.names = c("Estimate", "SE", "T", "P-value"), digits =  3) %>% column_spec(1,width = "0.8in") %>%
      column_spec(2,width =  "0.8in") %>%
      column_spec(3,width = "0.8in") %>%
      column_spec(4,width = "0.8in")  
```

### Python
#### a. Import Libraries and Datasets

```{python 1, engine.path = '/Users/Amy/anaconda3/bin/python', eval=FALSE}
# Set up: ----------------------------------------------------------------------
import xport
import pandas as pd
import numpy as np
import seaborn as sns
import math
import random
import matplotlib.pyplot as plt
from scipy.stats import t
from statsmodels.formula.api import ols
# Read in the data: ------------------------------------------------------------
## 'rb' mode - opens the file in binary format for reading
with open('HSQ_D.XPT', 'rb') as f:
    df_health = xport.to_dataframe(f)
with open('ALQ_D.XPT', 'rb') as f:
    df_alcohol = xport.to_dataframe(f)
with open('DEMO_D.XPT', 'rb') as f:
    df_demo = xport.to_dataframe(f)
```

#### b. Data Preparation
We first extracted key variables and filtered the targeted rows from three separate datasets. Then, we merged columns into one data frame and removed the  missing values. Finally, we factorize the variables of "sex", "education", and "health", as well as normaliza the variables of "alcohol", "age", and "pir".

```{python 2, engine.path = '/Users/Amy/anaconda3/bin/python', eval=FALSE}
# Data preparation: ------------------------------------------------------------
# Extract key columns
df_health = df_health.loc[df_health['HSD010'] <= 3, ['SEQN','HSD010']]
df_alcohol = df_alcohol.loc[df_alcohol['ALQ120Q'] <= 365, ['SEQN','ALQ120Q']]
df_demo = df_demo.loc[(df_demo.RIDAGEYR >= 21) & (df_demo.DMDEDUC2 <= 5), 
                      ['SEQN', 'RIAGENDR', 'RIDAGEYR', 'INDFMPIR', 'DMDEDUC2']]
# Merge key columns into one data frame
df = pd.merge(df_alcohol, df_health, on = 'SEQN')
df = pd.merge(df, df_demo, on = 'SEQN')
# Drop missing values
#df.isnull().sum()
df = df.dropna(axis = 0)
# Rename columns
df = df.rename(columns = {"SEQN": "id", "ALQ120Q": "alcohol", "HSD010": "health",
                          "RIAGENDR": "sex", "RIDAGEYR": "age", "INDFMPIR": "pir",
                          "DMDEDUC2": "edu"})
                          
# Normalize alcohol, age, and poverty income ratio(pir)
df.alcohol = (df.alcohol - np.mean(df.alcohol)) / np.std(df.alcohol)
df.age = (df.age - np.mean(df.age)) / np.std(df.age)
df.pir = (df.pir - np.mean(df.pir)) / np.std(df.pir)
# Factorize health and education
df.sex = df.sex.astype('category')
df.edu = df.edu.astype('category')
df.health = pd.factorize(df.health)[0]
```

#### c. OLS Model and Diagnosis
The initial OLS model regress alcohol on health, sex, age, poverty income ratio(pir), and education. The reference level of the model was the "good" health condition. The summary of the fitted model is shown below.

```{python 3, engine.path = '/Users/Amy/anaconda3/bin/python', eval=FALSE}
# Initial linear regression model: ---------------------------------------------
lmod = ols('alcohol ~ C(health) + C(sex) + age + pir + C(edu)', data = df).fit()
lmod.summary()
```

The model doesn't fit very well since most of the variables aren't significant at alpha level = .1 and the R-squared is only .004. 

```{python 4, engine.path = '/Users/Amy/anaconda3/bin/python', eval=FALSE}
# plot residual errors
fitted = lmod.fittedvalues
res = lmod.resid
plt.style.use('ggplot')
plt.scatter(fitted, res, color = "green", s = 10)
plt.hlines(y = 0, xmin = -0.16, xmax = 0.15, linewidth = 1, color = "red")
plt.title("Residuals vs Fitted")
plt.show()
```

In addition, the residual versus fitted plot presents the non-normality issue. In order to trust our model more and make the standard errors more robust, we resample the residuals to the estimate and adding them back to the fitted values.

#### d. Bootstrapping Residuals
The function is constructed to sample residuals from the initial OLS model with replacement. Then, by adding them back to the original fitted values, we created the new responses. Using those new responses, we fitted OLS model again with predictor values being fixed, but changing the responses.

```{python 5, engine.path = '/Users/Amy/anaconda3/bin/python', eval=FALSE}
# Function to resample residuals with replacement and fit new model: -----------
def boot_res(df, fitted, res):
    # input: 
    #   df - the original data for fitting initial model
    #   fitted - a array of fitted values from the initial model
    #   res  - a array of residuals from the initial model
    # output: 
    #   n_lmod.params - the coefficients of new model
    #   n_se - standard error for the additional analysis
    
    # sampling residauls with replacement
    b_res = np.random.choice(res, size = len(res), replace = True)
    n_se = math.sqrt( sum(b_res**2) / (df.shape[0] - df.shape[1] - 1) )
    
    # adding the resampled residuals back to the fitted values
    new_y = fitted + b_res
    
    # combine old predictors values with new responses
    X = df.iloc[:,2:]
    n_df = pd.concat([new_y, X], axis = 1)
    n_df.rename({0:'alcohol'}, axis = 1, inplace = True) 
    
    # fit new model
    n_lmod = ols('alcohol ~ C(health) + C(sex) + age + pir + C(edu)', data = n_df).fit()
    return(n_lmod.params, n_se)
# Test the function
#boot_res(df, fitted, res)
# Bootstrapping residuals 1000 times: ------------------------------------------
random.seed(506)
B = 1000
b = [boot_res(df, fitted, res) for i in range(B)]
b_coef = [lis[0] for lis in b]
# convert list to dataframe
b_df = pd.DataFrame(np.row_stack(np.array(b_coef)), 
                    columns = ['Intercept', 'health.1', 'health.2', 'sex.2', 'edu.2',
                               'edu.3', 'edu.4', 'edu.5', 'age', 'pir'])
# Compute SE for 1000 times bootstrap
b_se = b_df.std(axis = 0)
#print("Standard Error for each coefficient:", b_se)
# Plot the distribution of bootstrapping coefficients for "health" variable
#bh1 = sns.distplot(b_df.iloc[:,1])
#bh1.set_title('Coefficients Distribution of Health "very good"')
# Compute t-statistic
tval = np.array(lmod.params)[:] / np.array(b_se)
# Compute p-value
pval = t.sf(np.abs(tval), 1)
# Combine result into a dataframe
col = ["Estimate", "SE", "tStats", "pValue"]
rows = lmod.params.index.values
data = np.array([lmod.params, b_se, tval, pval])
data = np.transpose(data)
tbl = pd.DataFrame(data=data, columns=col, index=rows)
tbl
```

### Matlab

#### a. Data Prepartion

First, we load data, and select the variables mentioned in the begining. Then we merge three data sets into one with key variable "SEQN". After renaming variables, we do some data cleaning by filtering out invaild values such as missing values. The last step in this part is centralizing numeric variables and factorizing categorical variables. In particular, we set "3"(which represent "Good") in health condition variable as base.

```{matlab, eval = FALSE, include = TRUE}
% Title:  Matlab script for group project, STATS 506
% Author: Daxuan Deng
% Funtion: explore the relationship between health condition and drinking
%          alcohol, using data NHANES 2005-2006 data.
% Date: 11/30/2019

% load data
alq = xptread('ALQ_D.XPT');
demo = xptread('DEMO_D.XPT');
hsq = xptread('HSQ_D.XPT');

% select variables
alq = alq(:, {'SEQN', 'ALQ120Q'});
demo = demo(:, {'SEQN', 'RIAGENDR', 'RIDAGEYR', 'DMDEDUC2', 'INDFMPIR'});
hsq = hsq(:, {'SEQN', 'HSD010'});

% merge data
dt = join(alq, demo, 'Keys', 'SEQN');
dt = join(dt, hsq, 'Keys', 'SEQN');

% rename columns
dt.Properties.VariableNames = ...
["id", "alcohol", "sex", "yr", "edu", "pir", "health"];

% drop invalid values
dt = rmmissing(dt);
dt(dt.alcohol > 365, :) = [];
dt(dt.yr < 21, :) = [];
dt(dt.edu > 5, :) = [];
dt(dt.health > 3, :) = [];

% centralize and factorize
dt.alcohol = (dt.alcohol - mean(dt.alcohol)) ./ std(dt.alcohol);
dt.sex = categorical(dt.sex);
dt.yr = (dt.yr - mean(dt.yr)) ./ std(dt.yr);
dt.edu = categorical(dt.edu);
dt.pir = (dt.pir - mean(dt.pir)) ./ std(dt.pir);
dt.health = categorical(dt.health);
% set 'Good' as base level
dt.health = reordercats(dt.health, {'3','1','2'});
```

#### b. OLS and Diagnosis

Now we run regression. But the summary of coefficient estimates shows that we may not fit the model well. Furthermore, we plot the residuals, which reveals that our data is skewed. In this case, a residual bootstrap seems to be an appropriate choice.

```{matlab, eval = FALSE, include = TRUE}
% run OLS
md = fitlm(dt, 'alcohol ~ sex + yr + edu + pir + health');

% summary
md.Coefficients
```

```{r, include=FALSE}
ols = read.csv("ols.csv",row.names = 1)
kable(ols,digits = 4)
```


```{matlab, eval = FALSE, include = TRUE}
% extract fitted values and residuals 
fit = predict(md, dt(:, 3:7));
res = md.Residuals.Raw;
coef = md.Coefficients(:,1);

% plot residuals
plot(1:height(dt), res, 'x'), title('residuals of OLS')
```

#### c. residual bootstrap

In this part, first we resample residuals with replacement, and generate a new 'error' vector. Combining it with fitted value, we obtain a new response. Again, run regression and we get a new estimate of coefficient. With 1000 times bootstrapping, we get the empirical distribution, and from this we calculate the variance of estimate.

```{matlab, eval = FALSE, include = TRUE}
% bootstrap
rng(506);
nboot = 1000;

% resample residuals
func = @(x)x;
res_boot = bootstrp(nboot, func, res);

dt_boot = dt(:, 3:7);
beta_boot = zeros(nboot, 10);

for i=1:nboot
    % generate new response
    dt_boot.alcohol = fit + res_boot(i,:)';
    
    % fit new model
    md_boot = fitlm(dt_boot, 'alcohol ~ sex + yr + edu + pir + health');
    
    % extract new estimate
    beta_boot(i,:) = table2array(md_boot.Coefficients(:,1))';
end

% calculate std err
se = std(beta_boot);

% summary
result = coef;
result.se = se';
result.t = result.Estimate ./ result.se;
result.pvalue = 1-tcdf(abs(result.t),1);

result
```

```{r, include=FALSE}
ols = read.csv("boot.csv",row.names = 1)
kable(ols,digits = 4)
```


## Additional Analysis


## Results

```{r, include = FALSE}
summary(df$age)
summary(df$health)
summary(df$education)
```

The project includes 2791 participants (51.92% males and 48.08% females) with a mean age of 47.19 years. In terms of health condition, 12.37% participants in self-reported excellent health, 38.34 % in very good helath and 49.29% in good health. Descriptive characteristics of the participants were shwon in table below. 

 <need a descriptive summary table here>

With the simple lineare regresion, ther were non-constant error variance and non=normality distirbution for residuals according to model diagnostics.

With two approaches of bootstrapping regression (boottrapping regression residuals and our own creativity) to evaluate the effect of drinking alcohol on health condition on US healthy adults, we got similar results and both supported people whom in a better health condition would slightly drink more alcohol than people in fair good health condition. 

## Discussion
Both residual bootstrap and our original method fail to fit the data well, due to the skewness of data.

## References
http://faculty.washington.edu/yenchic/17Sp_403/Lec6-bootstrap_reg.pdf
http://users.stat.umn.edu/~helwig/notes/boot-Notes.pdf


